{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12349e96-8a32-4be8-a04c-856561f6ba6f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:200: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:200: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\USUARIO\\AppData\\Local\\Temp\\ipykernel_1656\\3907847984.py:200: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  for i, chunk_df in enumerate(pd.read_csv(file_path, sep='\\s+', header=None, names=['E', 'N', 'Z'], chunksize=chunk_size)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Configuración de Parámetros para Detección Sísmica ---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Por favor, introduce la ruta COMPLETA de tu archivo TXT de datos sísmicos (ej. C:\\datos\\sismo.txt):  F:\\gustavo\\Desktop\\PracticasProfecionalizantes_1_2025\\TRVA.201502.E.N.Z.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruta de archivo 'F:\\gustavo\\Desktop\\PracticasProfecionalizantes_1_2025\\TRVA.201502.E.N.Z.txt' encontrada. Procediendo...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Introduce la frecuencia de muestreo de tus datos en Hz (ej. 40.0):  40\n",
      "Introduce la fecha de inicio de la muestra (YYYY-MM-DD, ej. 2023-01-15):  2015-02-01\n",
      "Introduce la hora de inicio de la muestra (HH:MM:SS, ej. 10:30:00):  00:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha y hora de inicio configuradas: 2015-02-01 00:00:00\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Introduce la frecuencia INFERIOR del filtro de paso de banda en Hz (ej. 1.0):  1.0\n",
      "Introduce la frecuencia SUPERIOR del filtro de paso de banda en Hz (ej. 5.0):  5.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Componentes unificadas: True (fijo en este script a la magnitud vectorial).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Introduce el umbral de amplitud para la señal unificada y filtrada (ej. 0.1). Las amplitudes menores serán descartadas:  2.5\n",
      "Introduce la constante de calibración para la magnitud instrumental simplificada (ej. 2.0). ESTE VALOR REQUIERE CALIBRACIÓN EMPÍRICA:  2.0\n",
      "Introduce la duración mínima de un evento (en segundos) para ser registrado (ej. 0.5):  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calculando el tamaño de los bloques para dividir el archivo... ---\n",
      "El archivo tiene 96768200 registros.\n",
      "Se dividirá en aproximadamente 50 bloques de 1935364 registros cada uno.\n",
      "\n",
      "--- Configuración Final del Procesamiento ---\n",
      "  Archivo TXT de entrada: F:\\gustavo\\Desktop\\PracticasProfecionalizantes_1_2025\\TRVA.201502.E.N.Z.txt\n",
      "  Directorio de salida: F:\\gustavo\\Desktop\\PracticasProfecionalizantes_1_2025\n",
      "  Frecuencia de muestreo: 40.0 Hz\n",
      "  Fecha y hora de inicio de los datos: 2015-02-01 00:00:00\n",
      "  Filtro de paso de banda: 1.0-5.0 Hz (orden: 4)\n",
      "  Unificación de componentes (E,N,Z a Magnitud): Sí\n",
      "  Umbral de amplitud para filtrar ('Magnitude_Filtered'): 2.5\n",
      "  Constante de calibración Ml (simplificada): 2.0\n",
      "  Duración mínima de evento para registro: 10.0s\n",
      "  Tamaño de bloque para procesamiento: 1935364 muestras\n",
      "  Ventana STA (corta): 1.0s\n",
      "  Ventana LTA (larga): 10.0s\n",
      "  Umbral STA/LTA ON: 2.5\n",
      "  Umbral STA/LTA OFF: 0.8\n",
      "\n",
      "--- Iniciando el procesamiento del archivo... ---\n",
      "\n",
      "Procesando bloque 1 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 2 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 3 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 4 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 5 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 6 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 7 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 8 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 9 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 10 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 11 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 12 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 13 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 14 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 15 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 16 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 17 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 18 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 19 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 20 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 21 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 22 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 23 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 24 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 25 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 26 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 27 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 28 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 29 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 30 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 31 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 32 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 33 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 34 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 35 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 36 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 37 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 38 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 39 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 40 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 41 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 42 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 43 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 44 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 45 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 46 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 47 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 48 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 49 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 50 de aproximadamente 50...\n",
      "\n",
      "--- Concatenando todos los bloques procesados en un único DataFrame para la salida final... ---\n",
      "DataFrame consolidado tiene 96768200 filas.\n",
      "\n",
      "--- Generando el archivo de salida con datos sísmicos dentro de los eventos detectados... ---\n",
      "\n",
      "El archivo de salida con los datos sísmicos de los eventos detectados se ha guardado en:\n",
      "F:\\gustavo\\Desktop\\PracticasProfecionalizantes_1_2025\\datos_eventos_detectados.csv\n",
      "Total de registros de eventos detectados exportados: 415513.\n",
      "\n",
      "--- Procesamiento completado ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import obspy\n",
    "from obspy.core import Stream, Trace\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
    "import os\n",
    "import datetime\n",
    "import math # Necesitamos importar math para ceil\n",
    "\n",
    "# --- 1. Entrada de parámetros del usuario (mejoras en la interacción) ---\n",
    "print(\"--- Configuración de Parámetros para Detección Sísmica ---\")\n",
    "\n",
    "while True:\n",
    "    file_path = input(\"Por favor, introduce la ruta COMPLETA de tu archivo TXT de datos sísmicos (ej. C:\\\\datos\\\\sismo.txt): \")\n",
    "    # Normalizar la ruta para manejar barras invertidas/normales y asegurar que funcione en diferentes OS\n",
    "    file_path = os.path.normpath(file_path).strip('\"') # Eliminar comillas si el usuario las pega\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Ruta de archivo '{file_path}' encontrada. Procediendo...\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Error: La ruta '{file_path}' no existe o no se puede acceder.\")\n",
    "        print(\"Asegúrate de escribir la ruta correctamente y de que el archivo exista.\")\n",
    "\n",
    "# Obtener el directorio del archivo de entrada para la salida\n",
    "output_directory = os.path.dirname(file_path)\n",
    "output_event_data_filename = os.path.join(output_directory, \"datos_eventos_detectados.csv\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        sampling_rate_str = input(\"Introduce la frecuencia de muestreo de tus datos en Hz (ej. 40.0): \")\n",
    "        sampling_rate = float(sampling_rate_str)\n",
    "        if sampling_rate <= 0:\n",
    "            print(\"La frecuencia de muestreo debe ser un número positivo (mayor que 0).\")\n",
    "        else:\n",
    "            break\n",
    "    except ValueError:\n",
    "        print(\"Entrada inválida. Por favor, introduce un número válido para la frecuencia de muestreo (ej. 40.0).\")\n",
    "\n",
    "# --- Solicitar fecha y hora de inicio ---\n",
    "while True:\n",
    "    start_date_str = input(\"Introduce la fecha de inicio de la muestra (YYYY-MM-DD, ej. 2023-01-15): \")\n",
    "    start_time_str = input(\"Introduce la hora de inicio de la muestra (HH:MM:SS, ej. 10:30:00): \")\n",
    "    try:\n",
    "        start_datetime = datetime.datetime.strptime(f\"{start_date_str} {start_time_str}\", \"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"Fecha y hora de inicio configuradas: {start_datetime}\")\n",
    "        break\n",
    "    except ValueError:\n",
    "        print(\"Formato de fecha u hora inválido. Por favor, usa YYYY-MM-DD y HH:MM:SS.\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        freq_min_bandpass_str = input(\"Introduce la frecuencia INFERIOR del filtro de paso de banda en Hz (ej. 1.0): \")\n",
    "        freq_min_bandpass = float(freq_min_bandpass_str)\n",
    "        if freq_min_bandpass < 0:\n",
    "            print(\"La frecuencia mínima del filtro no puede ser negativa.\")\n",
    "        else:\n",
    "            break\n",
    "    except ValueError:\n",
    "        print(\"Entrada inválida. Por favor, introduce un número para la frecuencia mínima del filtro.\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        freq_max_bandpass_str = input(\"Introduce la frecuencia SUPERIOR del filtro de paso de banda en Hz (ej. 5.0): \")\n",
    "        freq_max_bandpass = float(freq_max_bandpass_str)\n",
    "        if freq_max_bandpass <= freq_min_bandpass:\n",
    "            print(\"La frecuencia superior debe ser mayor que la frecuencia inferior.\")\n",
    "        elif freq_max_bandpass >= (sampling_rate / 2):\n",
    "            print(f\"Advertencia: La frecuencia superior ({freq_max_bandpass} Hz) es igual o mayor que la frecuencia de Nyquist ({sampling_rate / 2} Hz).\")\n",
    "            print(\"Esto puede llevar a resultados inesperados (aliasing) o un filtrado ineficaz.\")\n",
    "            if input(\"¿Deseas continuar a pesar de esta advertencia? (s/n): \").lower() != 's':\n",
    "                continue\n",
    "            break # El usuario decidió continuar a pesar de la advertencia\n",
    "        else:\n",
    "            break\n",
    "    except ValueError:\n",
    "        print(\"Entrada inválida. Por favor, introduce un número para la frecuencia superior del filtro.\")\n",
    "\n",
    "# --- Parámetro para unificación de componentes (siempre True por diseño del script) ---\n",
    "unify_components = True # Las componentes E, N, Z siempre se unificarán en 'Magnitude'.\n",
    "print(f\"Componentes unificadas: {unify_components} (fijo en este script a la magnitud vectorial).\")\n",
    "\n",
    "# --- Umbral de magnitud para filtrar (aplicado a la señal unificada y filtrada) ---\n",
    "while True:\n",
    "    try:\n",
    "        magnitude_threshold_str = input(\"Introduce el umbral de amplitud para la señal unificada y filtrada (ej. 0.1). Las amplitudes menores serán descartadas: \")\n",
    "        magnitude_threshold = float(magnitude_threshold_str)\n",
    "        if magnitude_threshold < 0:\n",
    "            print(\"El umbral de amplitud no puede ser negativo.\")\n",
    "        else:\n",
    "            break\n",
    "    except ValueError:\n",
    "        print(\"Entrada inválida. Por favor, introduce un número para el umbral de amplitud.\")\n",
    "\n",
    "# --- Constante de calibración para la magnitud instrumental simplificada (Ml_Calculada_Simplificada) ---\n",
    "while True:\n",
    "    try:\n",
    "        ml_calibration_constant_str = input(\"Introduce la constante de calibración para la magnitud instrumental simplificada (ej. 2.0). ESTE VALOR REQUIERE CALIBRACIÓN EMPÍRICA: \")\n",
    "        ml_calibration_constant = float(ml_calibration_constant_str)\n",
    "        # Una constante de calibración Ml puede ser positiva o negativa dependiendo de la región y sismógrafo.\n",
    "        # No ponemos restricción de positivo aquí.\n",
    "        break\n",
    "    except ValueError:\n",
    "        print(\"Entrada inválida. Por favor, introduce un número para la constante de calibración Ml.\")\n",
    "\n",
    "# --- Duración mínima del evento en segundos ---\n",
    "while True:\n",
    "    try:\n",
    "        min_event_duration_str = input(\"Introduce la duración mínima de un evento (en segundos) para ser registrado (ej. 0.5): \")\n",
    "        min_event_duration = float(min_event_duration_str)\n",
    "        if min_event_duration <= 0:\n",
    "            print(\"La duración mínima del evento debe ser un número positivo (mayor que 0).\")\n",
    "        else:\n",
    "            break\n",
    "    except ValueError:\n",
    "        print(\"Entrada inválida. Por favor, introduce un número para la duración mínima del evento.\")\n",
    "\n",
    "# --- Parámetros fijos para STA/LTA y filtro ---\n",
    "# Estos parámetros son internos y se pueden ajustar directamente en el código si es necesario.\n",
    "corners = 4 # Número de esquinas para el filtro (mayor número = filtro más empinado)\n",
    "stalta_short = 1.0 # Ventana STA en segundos\n",
    "stalta_long = 10.0 # Ventana LTA en segundos\n",
    "thresh_on = 2.5 # Umbral para activar la detección\n",
    "thresh_off = 0.8 # Umbral para desactivar la detección\n",
    "\n",
    "\n",
    "# --- Cálculo dinámico del tamaño del chunk y validación ---\n",
    "print(\"\\n--- Calculando el tamaño de los bloques para dividir el archivo... ---\")\n",
    "try:\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Contar líneas, excluyendo posibles encabezados si tu TXT los tuviera.\n",
    "        # Asumimos que no hay encabezado por tu uso de names=['E', 'N', 'Z'] en pd.read_csv.\n",
    "        total_lines = sum(1 for line in f)\n",
    "    \n",
    "    if total_lines == 0:\n",
    "        raise ValueError(\"El archivo de datos está vacío.\")\n",
    "\n",
    "    # Número de partes deseado. Se podría hacer configurable también.\n",
    "    num_parts = 50 \n",
    "    chunk_size = int(math.ceil(total_lines / num_parts))\n",
    "    \n",
    "    if chunk_size == 0: # Si el archivo es muy pequeño\n",
    "        chunk_size = total_lines \n",
    "\n",
    "    print(f\"El archivo tiene {total_lines} registros.\")\n",
    "    print(f\"Se dividirá en aproximadamente {int(np.ceil(total_lines / chunk_size))} bloques de {chunk_size} registros cada uno.\")\n",
    "\n",
    "    # --- ADVERTENCIA CRÍTICA SOBRE EL TAMAÑO DEL CHUNK Y STA/LTA ---\n",
    "    min_samples_for_stalta_LTA_window = int(stalta_long * sampling_rate)\n",
    "    min_recommended_chunk_for_stalta = int(min_samples_for_stalta_LTA_window * 2.5) # Recomendación: al menos 2.5 veces la ventana LTA\n",
    "\n",
    "    if chunk_size < min_recommended_chunk_for_stalta:\n",
    "        print(f\"\\n¡¡¡ADVERTENCIA CRÍTICA!!!\")\n",
    "        print(f\"El tamaño de cada bloque ({chunk_size} muestras) es MUY PEQUEÑO para la ventana larga de STA/LTA ({stalta_long}s = {min_samples_for_stalta_LTA_window} muestras).\")\n",
    "        print(f\"Esto causará que el algoritmo STA/LTA NO SE EJECUTE CORRECTAMENTE o NO DETECTE EVENTOS en la mayoría de los bloques, especialmente cerca de los bordes.\")\n",
    "        print(f\"Se recomienda que el tamaño del bloque sea al menos {min_recommended_chunk_for_stalta} muestras para que STA/LTA funcione de manera fiable.\")\n",
    "        print(f\"Considera dividir el archivo en MENOS bloques (ej. {int(np.ceil(total_lines / min_recommended_chunk_for_stalta))} en lugar de {num_parts} para un chunk_size de {min_recommended_chunk_for_stalta}) o ajustar tus ventanas STA/LTA (stalta_long).\")\n",
    "        \n",
    "        while True:\n",
    "            confirm = input(\"¿Deseas continuar a pesar de esta advertencia? (s/n): \").lower()\n",
    "            if confirm == 's':\n",
    "                break\n",
    "            elif confirm == 'n':\n",
    "                print(\"Procesamiento cancelado por el usuario.\")\n",
    "                exit() # Sale del script\n",
    "            else:\n",
    "                print(\"Respuesta inválida. Por favor, introduce 's' o 'n'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al calcular el tamaño total de líneas o el tamaño del chunk: {e}\")\n",
    "    print(\"Asegúrate de que la ruta del archivo sea correcta y que el archivo no esté vacío.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "print(f\"\\n--- Configuración Final del Procesamiento ---\")\n",
    "print(f\"  Archivo TXT de entrada: {file_path}\")\n",
    "print(f\"  Directorio de salida: {output_directory}\")\n",
    "print(f\"  Frecuencia de muestreo: {sampling_rate} Hz\")\n",
    "print(f\"  Fecha y hora de inicio de los datos: {start_datetime}\")\n",
    "print(f\"  Filtro de paso de banda: {freq_min_bandpass}-{freq_max_bandpass} Hz (orden: {corners})\")\n",
    "print(f\"  Unificación de componentes (E,N,Z a Magnitud): Sí\")\n",
    "print(f\"  Umbral de amplitud para filtrar ('Magnitude_Filtered'): {magnitude_threshold}\")\n",
    "print(f\"  Constante de calibración Ml (simplificada): {ml_calibration_constant}\") \n",
    "print(f\"  Duración mínima de evento para registro: {min_event_duration}s\")\n",
    "print(f\"  Tamaño de bloque para procesamiento: {chunk_size} muestras\")\n",
    "print(f\"  Ventana STA (corta): {stalta_short}s\")\n",
    "print(f\"  Ventana LTA (larga): {stalta_long}s\")\n",
    "print(f\"  Umbral STA/LTA ON: {thresh_on}\")\n",
    "print(f\"  Umbral STA/LTA OFF: {thresh_off}\")\n",
    "\n",
    "print(\"\\n--- Iniciando el procesamiento del archivo... ---\")\n",
    "\n",
    "# --- Procesamiento por Chunks ---\n",
    "try:\n",
    "    start_time_current_chunk = start_datetime\n",
    "    \n",
    "    all_processed_chunks_for_final_df = [] # Lista para almacenar DataFrames de chunks procesados\n",
    "    detected_event_summary_list = [] # Lista para almacenar los detalles de los eventos detectados\n",
    "\n",
    "    # Lee el archivo por chunks usando pandas\n",
    "    for i, chunk_df in enumerate(pd.read_csv(file_path, sep='\\s+', header=None, names=['E', 'N', 'Z'], chunksize=chunk_size)):\n",
    "        print(f\"\\nProcesando bloque {i+1} de aproximadamente {int(np.ceil(total_lines / chunk_size))}...\")\n",
    "        \n",
    "        # --- Calcular 'timestamp' para el chunk actual ---\n",
    "        # El número de muestras en el chunk es len(chunk_df)\n",
    "        chunk_df['timestamp'] = [start_time_current_chunk + datetime.timedelta(seconds=j / sampling_rate) \n",
    "                                 for j in range(len(chunk_df))]\n",
    "        \n",
    "        # --- Actualizar el tiempo de inicio para el siguiente chunk ---\n",
    "        # Esto asegura la continuidad temporal entre bloques\n",
    "        if not chunk_df.empty: # Asegurarse de que el chunk no esté vacío\n",
    "            start_time_current_chunk = chunk_df['timestamp'].iloc[-1] + datetime.timedelta(seconds=1 / sampling_rate)\n",
    "        else:\n",
    "            # Si el chunk está vacío después de la lectura, el siguiente chunk comenzará en el mismo tiempo\n",
    "            # o en el tiempo anterior actualizado si hubiera habido datos. Esto previene un salto si un chunk está vacío.\n",
    "            print(f\"Advertencia: El chunk {i+1} no contenía datos válidos en la lectura inicial. La continuidad temporal podría verse afectada.\")\n",
    "\n",
    "        # --- Limpieza de datos no numéricos y unificación de componentes ---\n",
    "        for col in ['E', 'N', 'Z']:\n",
    "            chunk_df[col] = pd.to_numeric(chunk_df[col], errors='coerce') # Convierte a numérico, errores a NaN\n",
    "        chunk_df.dropna(subset=['E', 'N', 'Z'], inplace=True) # Elimina filas con NaN en E, N o Z\n",
    "\n",
    "        if chunk_df.empty:\n",
    "            print(f\"Advertencia: El bloque {i+1} quedó vacío después de la limpieza de datos no numéricos. Saltando a la siguiente...\")\n",
    "            continue # Salta al siguiente chunk si este está vacío\n",
    "\n",
    "        # Calcula la magnitud vectorial (norma L2)\n",
    "        chunk_df['Magnitude'] = np.sqrt(chunk_df['E']**2 + chunk_df['N']**2 + chunk_df['Z']**2)\n",
    "        data_to_filter = chunk_df['Magnitude'].values\n",
    "        \n",
    "        # --- Aplicar filtro pasa-banda usando Obspy Trace ---\n",
    "        trace = Trace(data=data_to_filter)\n",
    "        trace.stats.sampling_rate = sampling_rate\n",
    "        # Se detrae y se aplica filtro\n",
    "        trace.detrend(\"constant\") # Eliminar media\n",
    "        trace.detrend(\"linear\")   # Eliminar tendencia lineal\n",
    "        trace.filter('bandpass', freqmin=freq_min_bandpass, freqmax=freq_max_bandpass, corners=corners, zerophase=True)\n",
    "        \n",
    "        chunk_df['Magnitude_Filtered'] = trace.data # Asignar los datos filtrados de vuelta al DataFrame\n",
    "\n",
    "        # --- Filtrar el DataFrame por el umbral de amplitud (Magnitude_Filtered) ---\n",
    "        # Ahora, solo las filas que superan el umbral de amplitud serán consideradas para STA/LTA\n",
    "        current_chunk_filtered_by_amplitude = chunk_df[chunk_df['Magnitude_Filtered'] > magnitude_threshold].copy()\n",
    "        \n",
    "        if current_chunk_filtered_by_amplitude.empty:\n",
    "            print(f\"Advertencia: El bloque {i+1} no tiene datos que superen el umbral de amplitud ({magnitude_threshold}). Saltando detección STA/LTA para este bloque.\")\n",
    "            # Aunque no haya datos por encima del umbral para STA/LTA, aún queremos almacenar el chunk original completo\n",
    "            # con las columnas Magnitude y Magnitude_Filtered para el DataFrame final si no aplicamos el umbral para el dataframe final.\n",
    "            # Aquí, lo que se añadió a `all_processed_chunks_for_final_df` es el `current_chunk_processed` que es `chunk_df` original.\n",
    "            # Si se desea que el DataFrame final SÓLO contenga lo que superó el umbral, se usará `current_chunk_filtered_by_amplitude`\n",
    "            # He decidido mantener la adición de `chunk_df` completo para que el usuario pueda ver la señal completa filtrada.\n",
    "            all_processed_chunks_for_final_df.append(chunk_df) # Añadir el chunk original con las columnas Magnitude y Magnitude_Filtered\n",
    "            continue # Saltar a la siguiente iteración del loop, no hay datos para STA/LTA en este chunk\n",
    "\n",
    "        # Añadir el chunk procesado (con todas sus filas originales y las columnas añadidas)\n",
    "        # a la lista para el DataFrame final, si no se filtró todo.\n",
    "        all_processed_chunks_for_final_df.append(chunk_df)\n",
    "\n",
    "\n",
    "        # --- Aplicar STA/LTA en la señal filtrada por amplitud ---\n",
    "        data_for_stalta = current_chunk_filtered_by_amplitude['Magnitude_Filtered'].values\n",
    "        \n",
    "        # Validar si hay suficientes puntos para STA/LTA en el chunk actual\n",
    "        npts_long = int(stalta_long * sampling_rate) # Número de muestras para la ventana larga\n",
    "        npts_short = int(stalta_short * sampling_rate) # Número de muestras para la ventana corta\n",
    "\n",
    "        if len(data_for_stalta) < max(npts_long, npts_short) + 1: # Se necesita al menos la ventana más grande + 1 para el cálculo\n",
    "            print(f\"Advertencia: El bloque {i+1} después del filtrado por amplitud ({len(data_for_stalta)} muestras) es demasiado corto para STA/LTA (se necesitan al menos {max(npts_long, npts_short) + 1} muestras). Saltando detección para este bloque.\")\n",
    "            continue # Salta a la siguiente iteración, no se puede aplicar STA/LTA\n",
    "\n",
    "        cft = classic_sta_lta(data_for_stalta, npts_short, npts_long)\n",
    "        on_off = trigger_onset(cft, thresh_on, thresh_off)\n",
    "        \n",
    "        if on_off.size > 0:\n",
    "            # Los índices 'onset' y 'offset' se refieren al array `data_for_stalta`\n",
    "            # Necesitamos mapearlos de vuelta al DataFrame `current_chunk_filtered_by_amplitude`\n",
    "            \n",
    "            for onset_idx_in_stalta_data, offset_idx_in_stalta_data in on_off:\n",
    "                # Obtener los timestamps correspondientes del DataFrame `current_chunk_filtered_by_amplitude`\n",
    "                # que es el subconjunto de `chunk_df` que pasó el `magnitude_threshold`\n",
    "                \n",
    "                # Asegurarse de que los índices estén dentro de los límites\n",
    "                if onset_idx_in_stalta_data < len(current_chunk_filtered_by_amplitude) and \\\n",
    "                   offset_idx_in_stalta_data < len(current_chunk_filtered_by_amplitude):\n",
    "                    \n",
    "                    start_time_event = current_chunk_filtered_by_amplitude['timestamp'].iloc[onset_idx_in_stalta_data]\n",
    "                    end_time_event = current_chunk_filtered_by_amplitude['timestamp'].iloc[offset_idx_in_stalta_data]\n",
    "                    event_duration = (end_time_event - start_time_event).total_seconds()\n",
    "                    \n",
    "                    if event_duration >= min_event_duration:\n",
    "                        # Calcular la máxima amplitud del evento dentro del periodo detectado\n",
    "                        # Acceder a los datos originales del chunk_df usando los timestamps\n",
    "                        event_data_segment = chunk_df[\n",
    "                            (chunk_df['timestamp'] >= start_time_event) &\n",
    "                            (chunk_df['timestamp'] <= end_time_event)\n",
    "                        ]['Magnitude_Filtered']\n",
    "                        \n",
    "                        if not event_data_segment.empty:\n",
    "                            max_amplitude_in_event = event_data_segment.max()\n",
    "                            \n",
    "                            # Calcular la magnitud instrumental simplificada (Ml)\n",
    "                            # Se añade un pequeño valor para evitar log10(0) si la amplitud es 0\n",
    "                            ml_value = np.log10(max_amplitude_in_event + 1e-9) + ml_calibration_constant \n",
    "\n",
    "                            detected_event_summary_list.append({\n",
    "                                'Start_Time': start_time_event,\n",
    "                                'End_Time': end_time_event,\n",
    "                                'Duration_s': event_duration,\n",
    "                                'Max_Amplitude_Filtered': max_amplitude_in_event,\n",
    "                                'Ml_Calculada_Simplificada': ml_value\n",
    "                            })\n",
    "                        else:\n",
    "                            print(f\"Advertencia: Segmento de datos vacío para el evento detectado en {start_time_event}.\")\n",
    "                else:\n",
    "                    print(f\"Advertencia: Índices de STA/LTA fuera de rango en bloque {i+1}. Posiblemente un evento en el borde.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: El archivo '{file_path}' no fue encontrado. Verifica la ruta e inténtalo de nuevo.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error inesperado durante el procesamiento. Por favor, revisa tus datos o parámetros: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- Consolidar y Exportar Resultados ---\n",
    "\n",
    "final_processed_df_with_all_chunks = pd.DataFrame()\n",
    "if all_processed_chunks_for_final_df:\n",
    "    print(\"\\n--- Concatenando todos los bloques procesados en un único DataFrame para la salida final... ---\")\n",
    "    final_processed_df_with_all_chunks = pd.concat(all_processed_chunks_for_final_df).reset_index(drop=True)\n",
    "    print(f\"DataFrame consolidado tiene {len(final_processed_df_with_all_chunks)} filas.\")\n",
    "else:\n",
    "    print(\"\\nNo se procesó ningún bloque de datos. No se generará un DataFrame final.\")\n",
    "\n",
    "\n",
    "if not final_processed_df_with_all_chunks.empty and detected_event_summary_list:\n",
    "    print(\"\\n--- Generando el archivo de salida con datos sísmicos dentro de los eventos detectados... ---\")\n",
    "    \n",
    "    events_df_summary = pd.DataFrame(detected_event_summary_list)\n",
    "    event_data_rows_for_final_output = []\n",
    "\n",
    "    # Iterar a través de cada evento detectado para extraer los datos correspondientes\n",
    "    for index, event in events_df_summary.iterrows():\n",
    "        event_start = event['Start_Time']\n",
    "        event_end = event['End_Time']\n",
    "        event_duration = event['Duration_s']\n",
    "        max_amplitude = event['Max_Amplitude_Filtered']\n",
    "        ml_value = event['Ml_Calculada_Simplificada']\n",
    "\n",
    "        # Seleccionar las filas del DataFrame consolidado que caen dentro del rango de tiempo de este evento\n",
    "        data_in_event = final_processed_df_with_all_chunks[\n",
    "            (final_processed_df_with_all_chunks['timestamp'] >= event_start) & \n",
    "            (final_processed_df_with_all_chunks['timestamp'] <= event_end)\n",
    "        ].copy() \n",
    "\n",
    "        if not data_in_event.empty:\n",
    "            # Añadir metadatos del evento a cada fila de datos de ese evento\n",
    "            data_in_event['Fecha_Evento'] = event_start.strftime('%Y-%m-%d')\n",
    "            data_in_event['Hora_Inicio_Evento'] = event_start.strftime('%H:%M:%S.%f')[:-3] # ms\n",
    "            data_in_event['Hora_Fin_Evento'] = event_end.strftime('%H:%M:%S.%f')[:-3] # ms\n",
    "            data_in_event['Duracion_Evento_s'] = event_duration\n",
    "            data_in_event['Max_Amplitud_Filtrada'] = max_amplitude\n",
    "            data_in_event['Ml_Calculada_Simplificada'] = ml_value\n",
    "\n",
    "            # Dividir el timestamp original en 'Fecha_Lectura' y 'Hora_Lectura'\n",
    "            data_in_event['Fecha_Lectura'] = data_in_event['timestamp'].dt.strftime('%Y-%m-%d')\n",
    "            data_in_event['Hora_Lectura'] = data_in_event['timestamp'].dt.strftime('%H:%M:%S.%f')[:-3] # ms\n",
    "\n",
    "            # Definir las columnas deseadas y su orden para el archivo final\n",
    "            cols_to_keep = [\n",
    "                'Fecha_Evento', 'Hora_Inicio_Evento', 'Hora_Fin_Evento', 'Duracion_Evento_s',\n",
    "                'Max_Amplitud_Filtrada', 'Ml_Calculada_Simplificada', \n",
    "                'Fecha_Lectura', 'Hora_Lectura', \n",
    "                'E', 'N', 'Z', 'Magnitude', 'Magnitude_Filtered'\n",
    "            ]\n",
    "            \n",
    "            # Seleccionar y reordenar las columnas deseadas\n",
    "            data_in_event = data_in_event[cols_to_keep]\n",
    "            event_data_rows_for_final_output.append(data_in_event)\n",
    "\n",
    "    if event_data_rows_for_final_output:\n",
    "        final_output_df = pd.concat(event_data_rows_for_final_output).reset_index(drop=True)\n",
    "        final_output_df.to_csv(output_event_data_filename, index=False)\n",
    "        print(f\"\\nEl archivo de salida con los datos sísmicos de los eventos detectados se ha guardado en:\\n{output_event_data_filename}\")\n",
    "        print(f\"Total de registros de eventos detectados exportados: {len(final_output_df)}.\")\n",
    "    else:\n",
    "        print(\"\\nNo se encontraron datos sísmicos dentro de los eventos detectados que cumplan todos los criterios.\")\n",
    "else:\n",
    "    print(\"\\nNo se pudo generar el archivo de datos de eventos filtrados (no se detectaron eventos o el DataFrame consolidado está vacío).\")\n",
    "\n",
    "print(\"\\n--- Procesamiento completado ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad7766-c9ce-41ab-9877-3fb1f622eb42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
