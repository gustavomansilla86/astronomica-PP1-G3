{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea4bfc7b-0824-4eef-9c3b-040ef0fea141",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:190: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:190: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\USUARIO\\AppData\\Local\\Temp\\ipykernel_3476\\696250649.py:190: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  for i, chunk_df in enumerate(pd.read_csv(file_path, sep='\\s+', header=None, names=['E', 'N', 'Z'], chunksize=chunk_size)):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Por favor, introduce la ruta COMPLETA de tu archivo TXT de datos sísmicos sin comillas (ej. F:\\ruta\\al\\archivo.txt):  F:\\gustavo\\Desktop\\PracticasProfecionalizantes_1_2025\\TRVA.201502.E.N.Z.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruta de archivo 'F:\\gustavo\\Desktop\\PracticasProfecionalizantes_1_2025\\TRVA.201502.E.N.Z.txt' encontrada. Procediendo...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Introduce la frecuencia de muestreo de tus datos en Hz (ej. 40):  40\n",
      "Introduce la fecha de inicio de la muestra (YYYY-MM-DD, ej. 2023-01-15):  2015-02-01\n",
      "Introduce la hora de inicio de la muestra (HH:MM:SS.ffffff, ej. 10:30:00.000000):  00:00:00.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fecha y hora de inicio configuradas: 2015-02-01 00:00:00\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Introduce la frecuencia INFERIOR del filtro de paso de banda en Hz (ej. 1.0):  1.0\n",
      "Introduce la frecuencia SUPERIOR del filtro de paso de banda en Hz (ej. 5.0):  5.0\n",
      "¿Deseas unificar las componentes E, N, Z en una sola columna 'Magnitude'? (s/n):  S\n",
      "Introduce el umbral de magnitud (ej. 0.1) para filtrar la columna unificada/filtrada:  0.1\n",
      "Introduce la duración mínima de un evento (en segundos) para ser registrado (ej. 0.5):  7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculando el tamaño de los bloques para dividir el archivo en 50 partes...\n",
      "El archivo tiene 96768200 registros.\n",
      "Se dividirá en 50 bloques de aproximadamente 1935364 registros cada uno.\n",
      "\n",
      "--- Configuración final ---\n",
      "  Archivo TXT: F:\\gustavo\\Desktop\\PracticasProfecionalizantes_1_2025\\TRVA.201502.E.N.Z.txt\n",
      "  Frecuencia de muestreo: 40.0 Hz\n",
      "  Fecha y hora de inicio: 2015-02-01 00:00:00\n",
      "  Filtro de paso de banda: 1.0-5.0 Hz\n",
      "  Unificar componentes: Sí\n",
      "  Umbral de magnitud para filtro: 0.1\n",
      "  Duración mínima de evento para registro: 7.0s\n",
      "  Tamaño de bloque para procesamiento: 1935364 muestras (para 50 partes)\n",
      "  Ventana STA/LTA Corta: 1.0s\n",
      "  Ventana STA/LTA Larga: 10.0s\n",
      "  Umbral STA/LTA ON: 2.5\n",
      "  Umbral STA/LTA OFF: 0.8\n",
      "\n",
      "--- Procesando el archivo... ---\n",
      "\n",
      "Procesando bloque 1 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 2 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 3 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 4 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 5 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 6 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 7 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 8 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 9 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 10 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 11 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 12 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 13 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 14 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 15 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 16 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 17 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 18 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 19 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 20 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 21 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 22 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 23 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 24 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 25 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 26 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 27 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 28 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 29 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 30 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 31 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 32 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 33 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 34 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 35 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 36 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 37 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 38 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 39 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 40 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 41 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 42 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 43 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 44 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 45 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 46 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 47 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 48 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 49 de aproximadamente 50...\n",
      "\n",
      "Procesando bloque 50 de aproximadamente 50...\n",
      "\n",
      "--- Concatenando todos los bloques procesados en un DataFrame base... ---\n",
      "DataFrame base procesado tiene 48251240 filas.\n",
      "\n",
      "--- Generando el ÚNICO archivo de salida con datos sísmicos dentro de los eventos detectados... ---\n",
      "\n",
      "El ÚNICO archivo de salida con los datos sísmicos filtrados de los eventos detectados se ha guardado en:\n",
      "F:\\gustavo\\Desktop\\PracticasProfecionalizantes_1_2025\\TRVA.201502.E.N.Z_datos_eventos_detectados.csv\n",
      "\n",
      "--- Procesamiento completado ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import obspy\n",
    "from obspy.core import Stream, Trace\n",
    "from obspy.signal.trigger import classic_sta_lta, trigger_onset\n",
    "import os\n",
    "import datetime\n",
    "import math # Necesitamos importar math para ceil\n",
    "\n",
    "# --- 1. Entrada de parámetros del usuario ---\n",
    "while True:\n",
    "    file_path = input(\"Por favor, introduce la ruta COMPLETA de tu archivo TXT de datos sísmicos sin comillas (ej. F:\\\\ruta\\\\al\\\\archivo.txt): \")\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Ruta de archivo '{file_path}' encontrada. Procediendo...\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Error: La ruta '{file_path}' no existe o no se puede acceder.\")\n",
    "        print(\"Asegúrate de escribir la ruta correctamente y de que el archivo exista.\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        sampling_rate_str = input(\"Introduce la frecuencia de muestreo de tus datos en Hz (ej. 40): \")\n",
    "        sampling_rate = float(sampling_rate_str)\n",
    "        if sampling_rate <= 0:\n",
    "            print(\"La frecuencia de muestreo debe ser un número positivo.\")\n",
    "        else:\n",
    "            break\n",
    "    except ValueError:\n",
    "        print(\"Entrada inválida. Por favor, introduce un número para la frecuencia de muestreo.\")\n",
    "\n",
    "# --- Solicitar fecha y hora de inicio ---\n",
    "while True:\n",
    "    start_date_str = input(\"Introduce la fecha de inicio de la muestra (YYYY-MM-DD, ej. 2023-01-15): \")\n",
    "    start_time_str = input(\"Introduce la hora de inicio de la muestra (HH:MM:SS.ffffff, ej. 10:30:00.000000): \")\n",
    "    try:\n",
    "        start_datetime = datetime.datetime.strptime(f\"{start_date_str} {start_time_str}\", \"%Y-%m-%d %H:%M:%S.%f\")\n",
    "        print(f\"Fecha y hora de inicio configuradas: {start_datetime}\")\n",
    "        break\n",
    "    except ValueError:\n",
    "        print(\"Formato de fecha u hora inválido. Por favor, usa %Y-%m-%d y HH:MM:SS.ffffff.\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        freq_min_bandpass_str = input(\"Introduce la frecuencia INFERIOR del filtro de paso de banda en Hz (ej. 1.0): \")\n",
    "        freq_min_bandpass = float(freq_min_bandpass_str)\n",
    "        if freq_min_bandpass < 0:\n",
    "            print(\"La frecuencia mínima no puede ser negativa.\")\n",
    "        else:\n",
    "            break\n",
    "    except ValueError:\n",
    "        print(\"Entrada inválida. Por favor, introduce un número para la frecuencia mínima.\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        freq_max_bandpass_str = input(\"Introduce la frecuencia SUPERIOR del filtro de paso de banda en Hz (ej. 5.0): \")\n",
    "        freq_max_bandpass = float(freq_max_bandpass_str)\n",
    "        if freq_max_bandpass <= freq_min_bandpass:\n",
    "            print(\"La frecuencia superior debe ser mayor que la frecuencia inferior.\")\n",
    "        elif freq_max_bandpass >= (sampling_rate / 2):\n",
    "            print(f\"Advertencia: La frecuencia superior ({freq_max_bandpass} Hz) es igual o mayor que la frecuencia de Nyquist ({sampling_rate / 2} Hz).\")\n",
    "            print(\"Esto puede llevar a resultados inesperados. Considera una frecuencia superior menor.\")\n",
    "            if input(\"¿Deseas continuar de todos modos? (s/n): \").lower() != 's':\n",
    "                continue\n",
    "            break\n",
    "        else:\n",
    "            break\n",
    "    except ValueError:\n",
    "        print(\"Entrada inválida. Por favor, introduce un número para la frecuencia superior.\")\n",
    "\n",
    "# --- Parámetro para unificación de componentes ---\n",
    "while True:\n",
    "    unify_components_str = input(\"¿Deseas unificar las componentes E, N, Z en una sola columna 'Magnitude'? (s/n): \").lower()\n",
    "    if unify_components_str in ['s', 'n']:\n",
    "        unify_components = True if unify_components_str == 's' else False\n",
    "        break\n",
    "    else:\n",
    "        print(\"Respuesta inválida. Por favor, introduce 's' para sí o 'n' para no.\")\n",
    "\n",
    "# --- Umbral de magnitud para filtrar ---\n",
    "magnitude_threshold = None\n",
    "if unify_components:\n",
    "    while True:\n",
    "        try:\n",
    "            magnitude_threshold_str = input(\"Introduce el umbral de magnitud (ej. 0.1) para filtrar la columna unificada/filtrada: \")\n",
    "            magnitude_threshold = float(magnitude_threshold_str)\n",
    "            if magnitude_threshold < 0:\n",
    "                print(\"El umbral de magnitud no puede ser negativo.\")\n",
    "            else:\n",
    "                break\n",
    "        except ValueError:\n",
    "            print(\"Entrada inválida. Por favor, introduce un número para el umbral de magnitud.\")\n",
    "else:\n",
    "    print(\"Nota: No se solicitará umbral de magnitud ya que las componentes no se unificarán.\")\n",
    "\n",
    "# --- Duración mínima del evento en segundos ---\n",
    "while True:\n",
    "    try:\n",
    "        min_event_duration_str = input(\"Introduce la duración mínima de un evento (en segundos) para ser registrado (ej. 0.5): \")\n",
    "        min_event_duration = float(min_event_duration_str)\n",
    "        if min_event_duration < 0:\n",
    "            print(\"La duración mínima no puede ser negativa.\")\n",
    "        else:\n",
    "            break\n",
    "    except ValueError:\n",
    "        print(\"Entrada inválida. Por favor, introduce un número para la duración mínima.\")\n",
    "\n",
    "\n",
    "# --- Parámetros fijos para STA/LTA y filtro ---\n",
    "corners = 4\n",
    "stalta_short = 1.0\n",
    "stalta_long = 10.0 # Ventana LTA de 10 segundos\n",
    "thresh_on = 2.5\n",
    "thresh_off = 0.8\n",
    "\n",
    "\n",
    "# --- Cálculo dinámico del tamaño del chunk ---\n",
    "print(\"\\nCalculando el tamaño de los bloques para dividir el archivo en 50 partes...\")\n",
    "try:\n",
    "    with open(file_path, 'r') as f:\n",
    "        total_lines = sum(1 for line in f)\n",
    "    \n",
    "    if total_lines == 0:\n",
    "        raise ValueError(\"El archivo de datos está vacío.\")\n",
    "\n",
    "    # Calcula el tamaño del chunk para que haya 50 partes, usando ceil para asegurar que se cubran todas las líneas\n",
    "    chunk_size = int(math.ceil(total_lines / 50))\n",
    "    \n",
    "    # Asegura que el chunk_size no sea cero si el archivo es muy pequeño (menos de 50 líneas)\n",
    "    if chunk_size == 0: \n",
    "        chunk_size = total_lines # Si hay menos de 50 líneas, se procesa todo en un chunk.\n",
    "\n",
    "    print(f\"El archivo tiene {total_lines} registros.\")\n",
    "    print(f\"Se dividirá en 50 bloques de aproximadamente {chunk_size} registros cada uno.\")\n",
    "\n",
    "    # --- ADVERTENCIA CRÍTICA SOBRE EL TAMAÑO DEL CHUNK Y STA/LTA ---\n",
    "    min_samples_for_stalta_LTA_window = int(stalta_long * sampling_rate)\n",
    "    # Recomendamos que el chunk sea al menos 2-3 veces el tamaño de la ventana LTA para que funcione bien\n",
    "    min_recommended_chunk_for_stalta = int(min_samples_for_stalta_LTA_window * 2.5) \n",
    "\n",
    "    if chunk_size < min_recommended_chunk_for_stalta:\n",
    "        print(f\"\\n¡¡¡ADVERTENCIA CRÍTICA!!!\")\n",
    "        print(f\"El tamaño de cada bloque ({chunk_size} muestras) es MUY PEQUEÑO para la ventana larga de STA/LTA ({stalta_long}s = {min_samples_for_stalta_LTA_window} muestras).\")\n",
    "        print(f\"Esto causará que el algoritmo STA/LTA NO SE EJECUTE CORRECTAMENTE o NO DETECTE EVENTOS en la mayoría de los bloques.\")\n",
    "        print(f\"Se recomienda que el tamaño del bloque sea al menos {min_recommended_chunk_for_stalta} muestras para que STA/LTA funcione.\")\n",
    "        print(\"Considera dividir el archivo en MENOS bloques (ej. 5 o 10 en lugar de 50) o ajustar tus ventanas STA/LTA (stalta_long).\")\n",
    "        \n",
    "        while True:\n",
    "            confirm = input(\"¿Deseas continuar a pesar de esta advertencia? (s/n): \").lower()\n",
    "            if confirm == 's':\n",
    "                break\n",
    "            elif confirm == 'n':\n",
    "                print(\"Procesamiento cancelado por el usuario.\")\n",
    "                exit() # Sale del script\n",
    "            else:\n",
    "                print(\"Respuesta inválida. Por favor, introduce 's' o 'n'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al calcular el tamaño total de líneas o el tamaño del chunk: {e}\")\n",
    "    print(\"Asegúrate de que la ruta del archivo sea correcta y que el archivo no esté vacío.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "print(f\"\\n--- Configuración final ---\")\n",
    "print(f\"  Archivo TXT: {file_path}\")\n",
    "print(f\"  Frecuencia de muestreo: {sampling_rate} Hz\")\n",
    "print(f\"  Fecha y hora de inicio: {start_datetime}\")\n",
    "print(f\"  Filtro de paso de banda: {freq_min_bandpass}-{freq_max_bandpass} Hz\")\n",
    "print(f\"  Unificar componentes: {'Sí' if unify_components else 'No'}\")\n",
    "if unify_components:\n",
    "    print(f\"  Umbral de magnitud para filtro: {magnitude_threshold}\")\n",
    "print(f\"  Duración mínima de evento para registro: {min_event_duration}s\")\n",
    "print(f\"  Tamaño de bloque para procesamiento: {chunk_size} muestras (para 50 partes)\")\n",
    "print(f\"  Ventana STA/LTA Corta: {stalta_short}s\")\n",
    "print(f\"  Ventana STA/LTA Larga: {stalta_long}s\")\n",
    "print(f\"  Umbral STA/LTA ON: {thresh_on}\")\n",
    "print(f\"  Umbral STA/LTA OFF: {thresh_off}\")\n",
    "\n",
    "print(\"\\n--- Procesando el archivo... ---\")\n",
    "\n",
    "# --- Cargar datos y crear columna 'timestamp' ---\n",
    "try:\n",
    "    start_time_current_chunk = start_datetime\n",
    "    \n",
    "    # Lista para almacenar los DataFrames de chunks procesados\n",
    "    all_processed_chunks_for_final_df = [] \n",
    "    # Lista para almacenar los detalles de los eventos detectados (para usar en el filtrado final)\n",
    "    detected_event_summary_list = []\n",
    "\n",
    "    # Iterar a través del archivo por chunks\n",
    "    for i, chunk_df in enumerate(pd.read_csv(file_path, sep='\\s+', header=None, names=['E', 'N', 'Z'], chunksize=chunk_size)):\n",
    "        print(f\"\\nProcesando bloque {i+1} de aproximadamente {int(np.ceil(total_lines / chunk_size))}...\")\n",
    "        \n",
    "        # Calcular el timestamp para el chunk actual\n",
    "        chunk_df['timestamp'] = [start_time_current_chunk + datetime.timedelta(seconds=j / sampling_rate) \n",
    "                                 for j in range(len(chunk_df))]\n",
    "        \n",
    "        # Actualizar el tiempo de inicio para el siguiente chunk\n",
    "        start_time_current_chunk = chunk_df['timestamp'].iloc[-1] + datetime.timedelta(seconds=1 / sampling_rate)\n",
    "\n",
    "        # --- Unificar componentes si se solicitó ---\n",
    "        if unify_components:\n",
    "            chunk_df['Magnitude'] = np.sqrt(chunk_df['E']**2 + chunk_df['N']**2 + chunk_df['Z']**2)\n",
    "            data_to_filter = chunk_df['Magnitude'].values\n",
    "        else:\n",
    "            data_to_filter = chunk_df['E'].values \n",
    "\n",
    "        # --- Filtrar pasa-banda ---\n",
    "        trace = Trace(data=data_to_filter)\n",
    "        trace.stats.sampling_rate = sampling_rate\n",
    "        trace.filter('bandpass', freqmin=freq_min_bandpass, freqmax=freq_max_bandpass, corners=corners, zerophase=True)\n",
    "        \n",
    "        # Asignar los datos filtrados de vuelta al DataFrame\n",
    "        if unify_components:\n",
    "            chunk_df['Magnitude_Filtered'] = trace.data\n",
    "        else:\n",
    "            chunk_df['E_Filtered'] = trace.data\n",
    "\n",
    "        # --- Filtrar el DataFrame por el umbral de magnitud unificada (si aplica) ---\n",
    "        if unify_components and magnitude_threshold is not None:\n",
    "            current_chunk_processed = chunk_df[chunk_df['Magnitude_Filtered'] > magnitude_threshold].copy()\n",
    "        else:\n",
    "            current_chunk_processed = chunk_df.copy() \n",
    "\n",
    "        # Añadir el chunk procesado a la lista para el DataFrame final\n",
    "        if not current_chunk_processed.empty:\n",
    "            all_processed_chunks_for_final_df.append(current_chunk_processed)\n",
    "\n",
    "        # --- Preparar datos para STA/LTA ---\n",
    "        if unify_components:\n",
    "            data_for_stalta = current_chunk_processed['Magnitude_Filtered'].values\n",
    "        else:\n",
    "            data_for_stalta = current_chunk_processed['E_Filtered'].values\n",
    "            \n",
    "        # --- Aplicar STA/LTA ---\n",
    "        if data_for_stalta.size > 0:\n",
    "            npts_long = int(stalta_long * sampling_rate)\n",
    "            \n",
    "            if len(data_for_stalta) < npts_long:\n",
    "                # Esta advertencia es normal aquí si el chunk_size es pequeño. La advertencia principal ya se dio arriba.\n",
    "                pass \n",
    "            else:\n",
    "                cft = classic_sta_lta(data_for_stalta, int(stalta_short * sampling_rate), int(stalta_long * sampling_rate))\n",
    "\n",
    "                on_off = trigger_onset(cft, thresh_on, thresh_off)\n",
    "                \n",
    "                if on_off.size > 0:\n",
    "                    source_df_for_timestamps = current_chunk_processed\n",
    "\n",
    "                    for onset, offset in on_off:\n",
    "                        if onset < len(source_df_for_timestamps) and offset < len(source_df_for_timestamps):\n",
    "                            start_time_event = source_df_for_timestamps['timestamp'].iloc[onset]\n",
    "                            end_time_event = source_df_for_timestamps['timestamp'].iloc[offset]\n",
    "                            event_duration = (end_time_event - start_time_event).total_seconds()\n",
    "                            \n",
    "                            if event_duration >= min_event_duration:\n",
    "                                detected_event_summary_list.append({\n",
    "                                    'Start_Time': start_time_event,\n",
    "                                    'End_Time': end_time_event,\n",
    "                                    'Duration_s': event_duration,\n",
    "                                })\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: El archivo '{file_path}' no fue encontrado. Verifica la ruta.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error durante el procesamiento: {e}\")\n",
    "\n",
    "# --- Concatenar todos los chunks procesados en un único DataFrame base ---\n",
    "final_processed_df = pd.DataFrame()\n",
    "if all_processed_chunks_for_final_df:\n",
    "    print(\"\\n--- Concatenando todos los bloques procesados en un DataFrame base... ---\")\n",
    "    final_processed_df = pd.concat(all_processed_chunks_for_final_df).reset_index(drop=True)\n",
    "    print(f\"DataFrame base procesado tiene {len(final_processed_df)} filas.\")\n",
    "else:\n",
    "    print(\"\\nNo hay datos procesados para formar el DataFrame base (posiblemente todos fueron filtrados por magnitud o archivo vacío).\")\n",
    "\n",
    "\n",
    "# --- Filtrar el DataFrame base para generar el archivo final con datos dentro de los eventos ---\n",
    "if not final_processed_df.empty and detected_event_summary_list:\n",
    "    print(\"\\n--- Generando el ÚNICO archivo de salida con datos sísmicos dentro de los eventos detectados... ---\")\n",
    "    \n",
    "    events_df_summary = pd.DataFrame(detected_event_summary_list)\n",
    "    event_data_rows_for_final_output = []\n",
    "\n",
    "    # Iterar a través de cada evento detectado para extraer los datos correspondientes\n",
    "    for index, event in events_df_summary.iterrows():\n",
    "        event_start = event['Start_Time']\n",
    "        event_end = event['End_Time']\n",
    "        event_duration = event['Duration_s']\n",
    "\n",
    "        # Seleccionar las filas de final_processed_df que caen dentro del rango de tiempo de este evento\n",
    "        data_in_event = final_processed_df[\n",
    "            (final_processed_df['timestamp'] >= event_start) & \n",
    "            (final_processed_df['timestamp'] <= event_end)\n",
    "        ].copy() \n",
    "\n",
    "        if not data_in_event.empty:\n",
    "            # Añadir las columnas de información del evento a estas filas\n",
    "            data_in_event['Inicio_Evento'] = event_start\n",
    "            data_in_event['Fin_Evento'] = event_end\n",
    "            data_in_event['Duracion_Evento_s'] = event_duration\n",
    "            \n",
    "            # Decidir qué columna de magnitud/filtrada incluir y renombrarla si es necesario\n",
    "            if unify_components:\n",
    "                data_in_event['Magnitud_Filtrada_MI'] = data_in_event['Magnitude_Filtered']\n",
    "                cols_to_keep = ['E', 'N', 'Z', 'timestamp', 'Magnitud_Filtrada_MI', 'Inicio_Evento', 'Fin_Evento', 'Duracion_Evento_s']\n",
    "            else:\n",
    "                data_in_event['E_Filtrada'] = data_in_event['E_Filtered'] \n",
    "                cols_to_keep = ['E', 'N', 'Z', 'timestamp', 'E_Filtrada', 'Inicio_Evento', 'Fin_Evento', 'Duracion_Evento_s']\n",
    "\n",
    "            # Seleccionar y reordenar las columnas deseadas para el archivo final\n",
    "            data_in_event = data_in_event[cols_to_keep]\n",
    "            event_data_rows_for_final_output.append(data_in_event)\n",
    "\n",
    "    if event_data_rows_for_final_output:\n",
    "        final_event_data_df = pd.concat(event_data_rows_for_final_output).reset_index(drop=True)\n",
    "        # Nombre del único archivo de salida\n",
    "        output_event_data_filename = os.path.splitext(file_path)[0] + \"_datos_eventos_detectados.csv\"\n",
    "        final_event_data_df.to_csv(output_event_data_filename, index=False)\n",
    "        print(f\"\\nEl ÚNICO archivo de salida con los datos sísmicos filtrados de los eventos detectados se ha guardado en:\\n{output_event_data_filename}\")\n",
    "    else:\n",
    "        print(\"\\nNo se encontraron datos sísmicos dentro de los eventos detectados que cumplan todos los criterios después de los filtros.\")\n",
    "else:\n",
    "    print(\"\\nNo se pudo generar el archivo de datos de eventos filtrados (no se detectaron eventos o el DataFrame base está vacío).\")\n",
    "\n",
    "print(\"\\n--- Procesamiento completado ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f165dd29-dfe3-4bd9-92da-52a6d7f653af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
